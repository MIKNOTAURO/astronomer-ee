---
layout: page
title: Migrating DAGs
permalink: /guides/migrating-dags/
hide: true
---



## Install Astronomer CLI

> Note: we currently have two CLIs called `astro`, one for our Cloud Edition and
this new CLI for Enterprise Edition. We're working to merge them, but for the
time being, the Enterprise CLI is called `astro-cli`. If you're not using our
Cloud Edition CLI, you can alias `astro-cli` to `astro`.
>
> See the README in <https://github.com/astronomerio/astro-cli> for complete
installation instructions.

## Migrating DAGs

Before deploying to production you will want to test locally. Below is a series of steps to preparing both your local environment and production environment to migrate your existing DAGs.

### Choose some DAGs to migrate
Migrating a batch of DAGs can be overwhelming. There is usually a series of environment specific configuration (see next section) which can lead to errors when migrating a DAG and plugins. You will want to start migrating a DAG or two at a time in order to isolate and resolve errors.

To migrate a DAG locally, simply copy it into the `[projectRoot]/dags` directory. If you have any supporting plugins, you can copy them into the `[projectRoot]/plugins` directory.

### Migrate Your Airflow Environment
Mentioned above, there is often a lot of environment specific logic that your DAGs and Plugins rely on.

* Code Environment
  * Ensure your Python versions match
  * Ensure your Airflow versions match
* Airflow Environment
  * Add supporting [Airflow Connections](https://airflow.incubator.apache.org/concepts.html?highlight=connection#connections), if applicable
  * Add supporting [Airflow Variables](https://airflow.incubator.apache.org/concepts.html?highlight=connection#variables), if applicable
  * Add supporting [Airflow Pools](https://airflow.incubator.apache.org/concepts.html?highlight=connection#pools), if applicable

## Testing and Debugging

### Testing Locally
Once your DAGs and plugins are copied and your have verified that your environments are setup to match you can begin testing locally. To begin testing run `astro airflow start` from your project root directory once the [astro-cli](https://github.com/astronomerio/astro-cli) is installed.

For any errors occurring in the DAGs themselves, the errors will appear in a red bar across the top of the web UI.

Some errors will occur in the plugins, and at the time of writing those errors are not pushed to the error notification in the web UI. When this happens you may notice that DAGs fail to appear up in the web UI or are listed but grayed-out and unclickable. In this situation, you can troubleshoot the errors through `docker logs`.

* Run `docker ps` to find the webserver container name or ID
* Run `docker logs [CONTAINER]` to see the logs generated by the Airflow webserver

### Testing in Production
If you are satisfied with your DAGs running locally, you can then begin to [deploy DAGs](http://enterprise.astronomer.io/guides/deploying-dags-with-astro-cli/index.html) into your production instance. At this point, troubleshooting DAG errors through the web UI is the same as troubleshooting DAG errors locally.

Plugin errors, on the other hand, will need to be debugged slightly differently. First you will want to [install and configure kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) to point to your Kubernetes instance. After having done that you can pull down the webserver logs with the following steps.

* `kubectl get pods` to find the webserver pod
* `kubectl logs [POD]` to pull down the logs generated by the Airflow webserver
